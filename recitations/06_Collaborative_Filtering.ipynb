{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recitation - Collaborative Filtering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgK7_mMVeqde",
        "colab_type": "text"
      },
      "source": [
        "## Recommender systems\n",
        "\n",
        "Recommender systems are ***active information filtering systems*** that personalize the information coming to a user based on his interests, relevance of the information, etc.\n",
        "\n",
        "There are typically 2 ways that these systems are built.\n",
        "\n",
        "1. **Content based filtering**\n",
        "\n",
        "![The algorithm infers you like the brand duff!](https://miro.medium.com/max/1334/1*oYpMnPQFZaiZQizgVWBpoA.png)\n",
        "\n",
        "  * users are recommended items based on those they have already consumed. For example, if you bought a blue pen, amazon may recommend you a red pen as they both are similar. This is not the scope of our discussion today.\n",
        "  * Problems include creation of a filter bubble, recommending items you have already purchased\n",
        "\n",
        "\n",
        "2. **Collaborative filtering**\n",
        "\n",
        "![](https://miro.medium.com/max/1374/1*-Jr1l2rlj9SBcCzlDHtN5g.jpeg)\n",
        "\n",
        "  * CF accumulates customer product ratings, identifies customers with common ratings, and offers recommendations based on inter-customer comparisons. Itâ€™s based on the idea that people who agree in their evaluations of certain items in the past are likely to agree again in the future. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_yJn9tIJcdR",
        "colab_type": "text"
      },
      "source": [
        "## Collaborative Filtering\n",
        "\n",
        "There are 2 ways to build collaborative filtering systems.\n",
        "\n",
        "### Memory based CF\n",
        "\n",
        "* User based:\n",
        "  * User 1 rated movie A highly\n",
        "  * User 2 rated movie A highly -> User 1 and User 2 are similar.\n",
        "  * Hence, if User 1 rates movie B low, user 2 will also rate movie B low\n",
        "\n",
        "* Item-based:\n",
        "  * Instead of measuring the similarity between users, the item-based CF recommends items based on their similarity with the items that the target user rated\n",
        "\n",
        "  ![](https://miro.medium.com/max/1400/1*7bFc9R97Z4jKK6J2jaSUlw.jpeg)\n",
        "\n",
        "* Similarity Matrix - Here, item(i, j) represents the rating user i gives movie j\n",
        "![](https://hackernoon.com/hn-images/1*9TC6BrfxYttJwiATFAIFBg.png)\n",
        "\n",
        "* Similarity between users x and y is calculated using cosine similarity where xi is the rating user x gives movie i and yi is the rating user y gives movie i. If user x has rated a movie and user y has not, you can assume the rating to be 0, use a constant or even skip it.\n",
        "\n",
        "![](https://i.imgur.com/I9T81nG.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTHJ9VG_Jgos",
        "colab_type": "text"
      },
      "source": [
        "### Code Example - User based CF\n",
        "\n",
        "\n",
        "We use the [Surprise Scikit](http://surpriselib.com/) (Simple Python RecommendatIon System Engine) library which is a scikit-learn type collection of ML recommendation algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkMslo5mpKt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f6d82e64-e19d-47f0-e3ff-447dfa87c1fc"
      },
      "source": [
        "%pip install scikit-surprise\n",
        "from surprise import Dataset\n",
        "\n",
        "# Loads the builtin Movielens-100k data\n",
        "movielens = Dataset.load_builtin('ml-100k')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (0.15.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lqs3iePujOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from surprise import Reader\n",
        "from surprise import KNNWithMeans\n",
        "\n",
        "# This is the same data that was plotted for similarity earlier\n",
        "# with one new user \"E\" who has rated only movie 1\n",
        "ratings_dict = {\n",
        "    \"item\": [1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6],\n",
        "    \"user\": ['A', 'B', 'F', 'D', 'F', 'A', 'B', 'C', 'F', 'A', 'D', 'F', 'A', 'C', 'B', 'D', 'E'],\n",
        "    \"rating\": [2, 5, 4, 1, 5, 2, 4, 5, 4, 4, 5, 1, 5, 2, 1, 4, 2],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(ratings_dict)\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "# Loads Pandas dataframe\n",
        "data = Dataset.load_from_df(df[[\"user\", \"item\", \"rating\"]], reader)\n",
        "\n",
        "# To use user-based cosine similarity\n",
        "sim_options = {\n",
        "    \"name\": \"cosine\",\n",
        "    \"user_based\": True,\n",
        "}\n",
        "algo = KNNWithMeans(sim_options=sim_options)\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX5xG8zysJPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6049d58c-5263-4f9b-f468-11ee62a418de"
      },
      "source": [
        "# Train\n",
        "trainingSet = data.build_full_trainset()\n",
        "algo.fit(trainingSet)\n",
        "\n",
        "# Predict\n",
        "prediction = algo.predict('E', 1)\n",
        "print (prediction.est)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "3.6666666666666665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni_PLD-n4eHc",
        "colab_type": "text"
      },
      "source": [
        "### Model based CF\n",
        "\n",
        "\n",
        "* Typically uses matrix based factorization, which is useful for deriving meaningful inferences from sparse matrices, primarily through latent features.\n",
        "* Matrix factorization can be seen as breaking down a large matrix into a product of smaller ones. A matrix A with dimensions `m x n` can be reduced to a product of two matrices X and Y with dimensions `m x p` and `p x n` respectively.\n",
        "\n",
        "#### Concrete Example \n",
        "\n",
        "![](https://files.realpython.com/media/dimensionality-reduction.f8686dd52b9c.jpg)\n",
        "\n",
        "Here, the left matrix and top matrix map `m` users to `2` latent features and `n` movies to the same 2 latent features respectively. Here is an interpretation \n",
        "\n",
        "* Assume that in a user vector (u, v), u represents how much a user likes the Horror genre, and v represents how much they like the Romance genre.\n",
        "\n",
        "  * The user vector (2, -1) thus represents a user who likes horror movies and rates them positively and dislikes movies that have romance and rates them negatively.\n",
        "\n",
        "* Assume that in an item vector (i, j), i represents how much a movie belongs to the Horror genre, and j represents how much that movie belongs to the Romance genre.\n",
        "\n",
        "  * The movie (2.5, 1) has a Horror rating of 2.5 and a Romance rating of 1. Multiplying it by the user vector using matrix multiplication rules gives you (2 * 2.5) + (-1 * 1) = 4.\n",
        "\n",
        "* **So, the movie belonged to the Horror genre, and the user could have rated it 5, but the slight inclusion of Romance caused the final rating to drop to 4.**\n",
        "\n",
        "\n",
        "#### **Singular Value Decomposition**\n",
        "\n",
        "* Simply a way to factorize a matrix, most popularly used for collaborative filtering and recommendation algorithms. \n",
        "* Users RMSE as the metric to ensure that the closest approximation is reached. We essentially turn the recommendation problem into an optimization problem.\n",
        "* The canonical equation is as shown. The way you derive inferences is just like the example we discussed\n",
        "\n",
        "![](https://hackernoon.com/hn-images/1*haUDjEiQmG0RapR0SHos6Q.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "### Code Example - SVD based CF\n",
        "\n",
        "We use `GridSearchCV` in conjucntion with the `SVD` algorithm, to give us the most optimal results by minimizing RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yELX8WpACw2z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f55c856-060d-4b6a-fae5-5bf31993a5e1"
      },
      "source": [
        "from surprise import SVD\n",
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "\"\"\"\n",
        "lr_all is the learning rate for all parameters (how much the parameters are adjusted in each iteration)\n",
        "reg_all is the regularization term for all parameters, which is a penalty term added to prevent overfitting.\n",
        "\"\"\"\n",
        "param_grid = {\n",
        "    \"n_epochs\": [5, 10],\n",
        "    \"lr_all\": [0.002, 0.005],\n",
        "    \"reg_all\": [0.4, 0.6]\n",
        "}\n",
        "\n",
        "# Get the best params using GridSearchCV\n",
        "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\"], cv=3)\n",
        "gs.fit(data)\n",
        "best_params = gs.best_params[\"rmse\"]\n",
        "\n",
        "# Extract and train model with best params\n",
        "svd_algo = SVD(n_epochs=best_params['n_epochs'],\n",
        "               lr_all=best_params['lr_all'],\n",
        "               reg_all=best_params['reg_all'])\n",
        "svd_algo.fit(trainingSet)\n",
        "\n",
        "# Predict\n",
        "prediction = svd_algo.predict('E', 1)\n",
        "print (prediction.est)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.449428534520331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiePgqDmHMds",
        "colab_type": "text"
      },
      "source": [
        "## Drawbacks\n",
        "\n",
        "* Visibility - The main drawback of SVD is that there is no to little explanation to the reason that we recommend an item to an user. This can be a huge problem if users are eager to know why a specific item is recommended to them.\n",
        "* Missing values - In the case of SVD, it doesnâ€™t assume anything about missing values. So you need to give some missing value imputation for SVD. This might bring in unnecessary noise into the model.\n",
        "* Recommending new items - Collaborative filtering can lead to some problems like cold start for new items that are added to the list. Until someone rates them, they donâ€™t get recommended.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIcwewQeIyM0",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "* [Building collaborative filtering systems](https://realpython.com/build-recommendation-engine-collaborative-filtering/#model-based)\n",
        "* [Collaborative filtering using KNN](https://heartbeat.fritz.ai/recommender-systems-with-python-part-ii-collaborative-filtering-k-nearest-neighbors-algorithm-c8dcd5fd89b2)\n",
        "* [Collaborative filtering using SVD](https://heartbeat.fritz.ai/recommender-systems-with-python-part-iii-collaborative-filtering-singular-value-decomposition-5b5dcb3f242b)\n",
        "* [Beginner's guide to creating an SVD Recommender System](https://towardsdatascience.com/beginners-guide-to-creating-an-svd-recommender-system-1fd7326d1f65)"
      ]
    }
  ]
}